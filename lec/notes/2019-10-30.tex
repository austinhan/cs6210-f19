\documentclass[12pt, leqno]{article} %% use to set typesize
\include{common}

\newcommand{\uQ}{\underline{Q}}
\newcommand{\uR}{\underline{R}}

\begin{document}

\hdr{2019-10-30}

\section{Inverse iteration and the QR method}

When we discussed the power method, we found that we could improve
convergence by a spectral transformation that mapped the eigenvalue we
wanted to something with large magnitude (preferably much larger than
the other eigenvalues).  This was the {\em shift-invert} strategy.
We already know there is a connection leading from the power method
to orthogonal iteration to the QR method, which we can summarize with
a small number of formulas.  Let us see if we can follow the same
path to uncover a connection from inverse iteration (the power method
with $A^{-1}$, a special case of shift-invert in which the shift is zero) to QR.
If we call the orthogonal factors
in orthogonal iteration  $\uQ^{(k)}$ ($\uQ^{(0)} = I$) and the iterates
in QR iteration $A^{(k)}$, we have
\begin{align}
  A^{k}   &= \uQ^{(k)} \uR^{(k)} \label{orth-iter-rel} \\
  A^{(k)} &= (\uQ^{(k)})^* A (\uQ^{(k)}).
\end{align}
In particular, note that because $R^{(k)}$ are upper triangular,
\[
  A^{k} e_1 = (\uQ^{(k)} e_1) r^{(k)}_{11};
\]
that is, the first column of $\uQ^{(k)}$ corresponds to the $k$th
step of power iteration starting at $e_1$.  What happens when we
consider negative powers of $A$?  Inverting (\ref{orth-iter-rel}),
we find
\[
  A^{-k} = (\uR^{(k)})^{-1} (\uQ^{(k)})^*
\]
The matrix $\tilde{R}^{(k)} = (\uR^{(k)})^{-1}$ is again upper triangular;
and if we look carefully, we can see in this fact another power iteration:
\[
  e_n^* A^{-k} = e_n^* \tilde{R}^{(k)} (\uQ^{(k)})^*
              = \tilde{r}^{(k)}_{nn} (\uQ^{(k)} e_n)^*.
\]
That is, the last column of $\uQ^{(k)}$ corresponds to a power iteration
converging to a {\em row} eigenvector of $A^{-1}$.

\section{Shifting gears}

The connection from inverse iteration to orthogonal iteration (and
thus to QR iteration) gives us a way to incorporate the shift-invert
strategy into QR iteration: simply run QR on the matrix $A-\sigma I$,
and the $(n,n)$ entry of the iterates (which corresponds to a Rayleigh
quotient with an increasingly-good approximate row eigenvector) should
start to converge to $\lambda - \sigma$, where $\lambda$ is the
eigenvalue nearest $\sigma$.  Put differently, we can run the
iteration:
\begin{align*}
  Q^{(k)} R^{(k)} &= A^{(k-1)} - \sigma I \\
  A^{(k)} &= R^{(k)} Q^{(k)} + \sigma I.
\end{align*}
If we choose a good shift, then the lower right corner entry of
$A^{(k)}$ should converge to the eigenvalue closest to $\sigma$ in
fairly short order, and the rest of the elements in the last row
should converge to zero.

The shift-invert power iteration converges fastest when we choose a
shift that is close to the eigenvalue that we want.  We can do even
better if we choose a shift {\em adaptively}, which was the basis for
running Rayleigh quotient iteration.  The same idea is the basis
for the {\em shifted QR iteration}:
\begin{align}
  Q^{(k)} R^{(k)} &= A^{(k-1)} - \sigma_k I \label{sqr-1} \\
  A^{(k)} &= R^{(k)} Q^{(k)} + \sigma_k I. \label{sqr-2}
\end{align}
This iteration is equivalent to computing
\begin{align*}
  \uQ^{(k)} \uR^{(k)} &= \prod_{j=1}^n (A-\sigma_j I) \\
  A^{(k)} &= (\uQ^{(k)})^* A (\uQ^{(k)}) \\
  \uQ^{(k)} &= Q^{(k)} Q^{(k-1)} \ldots Q^{(1)}.
\end{align*}

What should we use for the shift parameters $\sigma_k$?  A natural
choice is to use $\sigma_k = e_n^* A^{(k-1)} e_n$, which is the same
as $\sigma_k = (\uQ^{(k)} e_n)^* A (\uQ^{(k)} e_n)$, the Rayleigh quotient
based on the last column of $\uQ^{(k)}$.  This simple shifted QR iteration
is equivalent to running Rayleigh iteration starting from an initial vector
of $e_n$, which we noted before is locally quadratically convergent.

\section{Double trouble}

The simple shift strategy we described in the previous section gives
{\em local} quadratic convergence, but it is not {\em globally} convergent.
%% For example, suppose $P$ is a cyclic permutation matrix, i.e.
%% \[
%%   P = \begin{bmatrix}
%%         0 & 0 & \cdots & 0 & 1 \\
%%         1 & 0 & \cdots & 0 & 0 \\
%%         0 & 1 & \cdots & 0 & 0 \\
%%         \vdots & \ddots & \ddots & \vdots & \vdots \\
%%         0 & 0 & \cdots & 1 & 0
%%      \end{bmatrix}
%% \]
%% The matrix $P$ is a {\em fixed point} for the shifted QR iteration
%% (\ref{sqr-1})--(\ref{sqr-2}) when we choose
%% $\sigma_k = e_n^* A^{(k)} e_n = 0$.
%
As a particularly pesky example, consider what happens if we
want to compute a complex conjugate pair of eigenvalues of a real
matrix.  With our simple shifting strategy, 
%the iteration (\ref{sqr-1})--(\ref{sqr-2}) will 
the QR iteration
never produce a complex iterate, a
complex shift, or a complex eigenvalue.  The best we can hope for is
that our initial shift is closer to both eigenvalues in the conjugate
pair than it is to anything else in the spectrum; in this case, we
will most likely find that the last two columns of $\uQ^{(k)}$ are
converging to a basis for an {\em invariant row subspace} of $A$,
and the corresponding eigenvalues are the eigenvalues of the trailing
2-by-2 sub-block.

Fortunately, we know how to compute the eigenvalues of a 2-by-2
matrix!  This suggests the following shift strategy: let $\sigma_k$ be
one of the eigenvalues of $A^{(k)}(n-1:n,n-1:n)$.  Because this 2-by-2
problem can have complex roots even when the matrix is real, this
shift strategy allows the possibility that we could converge to
complex eigenvalues. On the other hand, if our original matrix is
real, perhaps we would like to consider the {\em real} Schur form, in
which $U$ is a real matrix and $T$ is block diagonal with 1-by-1 and
2-by-2 diagonal blocks that correspond, respectively, to real and
complex eigenvalues.  If we shift with {\em both} roots of
$A^{(k)}(n-1:n,n-1:n)$, equivalent to computing
\begin{align*}
  Q^{(k)} R^{(k)} &= (A^{(k-1)} - \sigma_{k+} I)(A^{(k-1)} - \sigma_{k-}) \\
  A^{(k)} &= (Q^{(k)})^* A^{(k-1)} Q^{(k)}. \label{sqr-2}
\end{align*}
There is one catch here: even if we started with $A^{(0)}$ in Hessenberg
form, it is unclear how to do this double-shift step in $O(n^2)$ time!

The following fact will prove our salvation: if we $Q$ and $V$ are
both orthogonal matrices and $Q^T A Q$ and $V^T A V$ are both
(unreduced) Hessenberg\footnote{ An unreduced Hessenberg matrix has no
  zeros on the first subdiagonal.  })  and the first column of $Q$ is
the same as the first column of $V$, then all successive columns of
$Q$ are unit scalar multiples of the corresponding columns of $V$.
This is the {\em implicit Q theorem}.  Practically, it means that we
can do any sort of shifted QR step we would like in the following way:
\begin{enumerate}
\item
  Apply as a similarity any transformations in the QR decomposition
  that affect the leading submatrix (1-by-1 or 2-by-2).
\item
  Restore the resulting matrix to Hessenberg form without further
  transformations to the leading submatrix.
\end{enumerate}
In the first step, we effectively compute the first column of $Q$;
in the second step, we effectively compute the remaining columns.
Certainly we compute {\em some} transformation with the right leading
column; and the implicit Q theorem tells us that any such transformation
is basically the one we would have computed with an ordinary QR step.

Last time, we discussed the Wilkinson strategy of choosing as a shift
one of the roots of the trailing 2-by-2 submatrix of $A^{(k)}$ (the
one closest to the final entry).  We also noted that if we want to
convert to {\em real} Schur form, the Wilkinson shift has the distinct
disadvantage that it might launch us into the complex plane.  The
Francis shift strategy is to simultaneously apply a complex conjugate
pair of shifts, essentially computing two steps together:
\begin{align*}
  Q^{(k)} R^{(k)} &= (A^{(k-1)}-\sigma_k I)(A^{(k-1)}-\bar{\sigma}_k I) \\
                &= (A^{(k-1)})^2 - 2\Re(\sigma_k) A^{(k-1)} + |\sigma_k|^2 I \\
  A^{(k)}        &= (Q^{(k)})^* A^{(k-1)} (Q^{(k)}).
\end{align*}
When the Wilkinson shift is real, we let $\sigma_k$ be the same as the
Wilkinson shift; when the Wilkinson strategy leads to a conjugate pair
of possible shifts, we use both, maintaining efficiency by doing the
steps {\em implicitly}.  Let's now make this implicit magic a little
more explicit by building code for an implicit double-shift QR step.

Our first step will be to construct the polynomial associated with the
Francis double-shift.  In the case where the trailing 2-by-2 submatrix
(or 2-by-2 block Rayleigh quotient, if one prefers) has a complex pair
of eigenvalues, we just use its characteristic polynomial.  Otherwise,
we use the polynomial associated with two steps with a Wilkinson shift.

\lstinputlisting{code/eigen/francis_poly.m}

The code {\tt francis\_poly} gives us coefficients $b_k$ and $c_k$ for a
quadratic function $s_k(z) = z^2 + b_k z + c_k$.  We now want to compute
\begin{align*}
  Q^{(k)} R^{(k)} &= s_k(A^{(k-1)}) = (A^{(k-1)})^2 + b_k A^{(k-1)} + c_k I \\
  A^{(k)}        &= (Q^{(k)})^* A^{(k-1)} (Q^{(k)}).
\end{align*}
The trick is to realize that all the iterates $A^{(k)}$ are Hessenberg,
and the Hessenberg form for a matrix is usually unique (up to signs).
Therefore, we compute the first Householder transformation $W$ in a
QR factorization of $s_k(A^{(k)}$ explicitly.  The first column of $Q^{(k)}$
is the same as the first column of $W$.  The remaining columns of $Q^{(k)}$
can be determined by the requirement that $A^{(k)}$ is in Hessenberg form.
We compute them implicitly by applying the usual Hessenberg reduction algorithm
to $B = WA^{(k-1)}W$, taking advantage of the fact that $B$ has special
structure to do $O(n^2)$ work.  Each step of the reduction moves a ``bulge''
down the diagonal by one.

\lstinputlisting{code/eigen/hessqr_francis.m}

In the LAPACK codes, the Francis double-shift strategy is mixed with
some ``exceptional shifts'' that occur every few iterations.
These exceptional shifts serve to keep the algorithm from getting
stuck in certain pathological situations (e.g. a cyclic permutation
matrix).

\section{Deflation}

A sequence of implicit doubly-shifted QR steps with the Francis shift
will usually give us rapid convergence of a trailing 1-by-1 or 2-by-2
submatrix to a block of a Schur factorization.  As this happens,
the trailing row (or two rows) becomes very close to zero.  When the
values in these rows are close enough to zero, we {\em deflate} by
setting them equal to zero.  This corresponds to a small perturbation
to the original problem.

The following code converts a Hessenberg matrix to a block upper triangular
matrix with 1-by-1 and 2-by-2 blocks.  To reduce this matrix further to
real Schur form, we would need to make an additional pass to further
reduce any 2-by-2 block with real eigenvalues into a pair of 1-by-1 blocks.

\lstinputlisting{code/eigen/hessqr.m}

More careful deflation criteria are usually used in practice;
see the book.  This criterion at least corresponds to small normwise
perturbations to the original problem, but it may result in less
accurate estimates of small eigenvalues than we could obtain with
a more aggressive criterion.

\section{Stability of the method}

Each step of the implicitly double-shifted QR iteration changes the
matrix only with orthogonal transformations (which are perfectly conditioned)
or deflations.  Hence, the QR iteration is backward stable.  However,
this is {\em not} the same as saying that the method is forward stable!
For forward stability, the conditioning of the eigenvalues is critical,
and multiple (or nearly multiple) eigenvalues of multiplicity $m$
usually inherit an $O(\epsilon^{1/m})$ error, as we saw in our earlier
discussion of sensitivity.

The intermediate computations in the QR code as given above are prone to
scaling problems, and so the basic QR codes in LAPACK ({\tt dlahqr})
uses a more careful construction of a scaled copy of the first Householder
transformation.

\section{The state of the art}

The current state of the art in QR iterations is the LAPACK code {\tt
  dgehqr} written by Ralph Byers, which is based on an award-winning
set of papers by Braman, Byers, and Mathias.  This code uses the following
general strategy:
\begin{enumerate}
\item
  Run the basic QR iteration to find the eigenvalues of a trailing $b \times b$
  submatrix.  Apply the transformations to the whole matrix, resulting in
  a ``spike'' to the left of the triangularized portion.
\item
  Look for converged eigenvalues in the trailing submatrix by analyzing
  the ``spike'' to find small elements.  Deflate any eigenvalues found
  (and there may be several).  This is called {\em aggressive early deflation}.
\item
  Use several of the remaining eigenvalues from the Rayleigh quotient block
  as a sequence of successive shifts.  These can be run simultaneously by
  chasing a sequence of closely-spaced bulges down the main diagonal.
  The similarity transformations associated are applied in a blocky way
  to get good cache performance.
\end{enumerate}

\end{document}
