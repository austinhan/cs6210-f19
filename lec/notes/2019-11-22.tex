\documentclass[12pt, leqno]{article} %% use to set typesize
\include{common}

\begin{document}

\hdr{2019-11-22}

\section{Arnoldi}

Krylov subspaces are good spaces for approximation schemes.  But the
power basis (i.e.~the basis $A^j b$ for $j = 0, \ldots, k-1$) is not
good for numerical work.  The vectors in the power basis tend to
converge toward the dominant eigenvector, and so the power basis quickly
becomes ill-conditioned.  We would much rather have orthonormal bases
for the same spaces.  This is where the Arnoldi iteration and its kin
come in.

Each step of the Arnoldi iteration consists of two pieces:
\begin{itemize}
\item
  Compute $Aq_k$ to get a vector in the space of dimension $k+1$
\item
  Orthogonalize $Aq_k$ against $q_1, \ldots, q_k$ using Gram-Schmidt.
  Scale the remainder to unit length.
\end{itemize}
If we keep track of the coefficients in the Gram-Schmidt process
in a matrix $H$, we have
\[
  h_{k+1,k} q_{k+1} = Aq_k - \sum_{j=1}^{k} q_j h_{jk}
\]
where $h_{jk} = q_j^T A q_k$ and $h_{k+1,k}$ is the normalization
constant.  Rearranging slightly, we have
\[
  Aq_k = \sum_{j=1}^{k+1} q_j h_{jk}.
\]
Defining $Q_k = \begin{bmatrix} q_1 & q_2 & \ldots & q_k \end{bmatrix}$,
we have the {\em Arnoldi decomposition}
\[
  AQ_k = Q_{k+1} \bar{H}_k, \quad
  \bar{H}_k =
  \begin{bmatrix}
    h_{11} & h_{12} & \ldots & h_{1k} \\
    h_{21} & h_{22} & \ldots & h_{2k} \\
           & h_{32} & \ldots & h_{3k} \\
           &        & \ddots & \vdots \\
           &        &        & h_{k+1,k}
  \end{bmatrix} \in \bbR^{(k+1) \times k}.
\]
\begin{figure}
\lstinputlisting{code/iter/arnoldi.m}
\caption{The standard Arnoldi algorithm.}
\label{arnoldi-1}
\end{figure}

\begin{figure}
\lstinputlisting{code/iter/arnoldi2.m}
\caption{The Arnoldi algorithm with re-orthogonalization.}
\label{arnoldi-2}
\end{figure}

The Arnoldi {\em decomposition} is simply the leading $k$ columns of an
upper Hessenberg reduction of the original matrix $A$.  The Arnoldi
{\em algorithm} is the interlaced multiply-and-orthogonalize process
used to obtain the decomposition (Figure~\ref{arnoldi-1}).
Unfortunately, the modified Gram-Schmidt algorithm --- though more
stable than classical Gram-Schmidt! --- is nonetheless unstable in
general.  The sore point occurs when we start with a matrix that lies
too near the span of the vectors we orthogonalize against; in this case,
by the time we finish orthogonalizing against previous vectors, we have
cancelled away so much of the original vector that what is left may be
substantially contaminated by roundoff.  For this reason, the
``twice is enough'' reorthogonalization process of Kahan and Parlett
is useful: this says that if the remainder after orthogonalization is
too small compared to what we started with, then we should perhaps
refine our computation by orthogonalizing the remainder again.
We show this process in Figure~\ref{arnoldi-2}.

\section{Lanczos}

Now suppose that $A$ is a symmetric matrix.  In this case, the Arnoldi
decomposition takes a special form: the upper Hessenberg matrix is now
a symmetric upper Hessenber matrix (aka a tridiagonal), and we dub
the resulting decomposition the {\em Lanczos} decomposition:
\[
  AQ_k = Q_{k+1} \bar{T}_k, \quad
  T_k =
  \begin{bmatrix}
    \alpha_1 & \beta_1 \\
    \beta_1 & \alpha_2 & \beta_2 \\
            & \beta_2 & \alpha_3 & \beta_3 \\
            & & \ddots & \ddots & \ddots \\
            & & & \beta_{k-2} & \alpha_{k-1} & \beta_{k-1} \\
            & & & & \beta_{k-1} & \alpha_k \\
            & & & & & \beta_k
  \end{bmatrix}
\]
The Lanczos algorithm is the specialization of the Arnoldi algorithm
to the symmetric case.  In exact arithmetic, the tridiagonal form of
the coefficient matrix allows us to do only a constant amount of
orthogonalization work at each step (Figure~\ref{lanczos}).

\begin{figure}
\lstinputlisting{code/iter/lanczos.m}
\caption{Lanczos iteration.  Note that in floating point, the columns
  of $Q$ will lose orthogonality.}
\label{lanczos}
\end{figure}

Sadly, the Lanczos algorithm in floating point behaves rather differently
from the algorithm in exact arithmetic.  In particular, the iteration
tends to ``restart'' periodically as the space starts to get very good
approximations of eigenvectors.  One can deal with this via full
reorthogonalization, as with the Arnoldi iteration; but then the method
loses the luster of low cost, as we have to orthogonalize against several
vetors periodically.  An alternate {\em selective orthogonalization}
strategy proposed by Parlett and Scott lets us orthogonalize only against
a few previous vectors, which are associated with converged eigenvector
approximations (Ritz vectors).  But, as we shall see, such orthogonalization
is mostly useful when we want to solve eigenvalue problems.  For linear
systems, it tends not to be necessary.

\end{document}
